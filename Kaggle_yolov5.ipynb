{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 链接:https://pan.baidu.com/s/1YI9gJSeJS97_-NDpJ-sN7g  密码:fcj6\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import shutil as sh\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/global-wheat-detection/train.csv')\n",
    "bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
    "    df[column] = bboxs[:,i]\n",
    "df.drop(columns=['bbox'], inplace=True)\n",
    "df['x_center'] = df['x'] + df['w']/2\n",
    "df['y_center'] = df['y'] + df['h']/2\n",
    "df['classes'] = 0\n",
    "df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id      x      y      w     h  x_center  y_center  classes\n",
       "0  b6ab77fd7  834.0  222.0   56.0  36.0     862.0     240.0        0\n",
       "1  b6ab77fd7  226.0  548.0  130.0  58.0     291.0     577.0        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(set(df.image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'train'\n",
    "if True:\n",
    "    for fold in [0,1,2,3,4]:\n",
    "        val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\n",
    "        for name,mini in df.groupby('image_id'):\n",
    "            if name in val_index:\n",
    "                path2save = 'val2017/'\n",
    "            else:\n",
    "                path2save = 'train2017/'\n",
    "            if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\n",
    "                os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n",
    "            with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n",
    "                row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n",
    "                row = row/1024\n",
    "                row = row.astype(str)\n",
    "                for j in range(len(row)):\n",
    "                    text = ' '.join(row[j])\n",
    "                    f.write(text)\n",
    "                    f.write(\"\\n\")\n",
    "            if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\n",
    "                os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n",
    "            sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 512 --batch 2 --epochs 100 --data ./data/wheat0.yaml --cfg ./models/yolov5x.yaml --name yolov5x_fold0 --weights ./weights/yolov5x.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 512 --batch 2 --epochs 100 --data ./gwdconfig/wheat1.yaml --cfg ./gwdconfig/yolov5x.yaml --name yolov5x_fold1 --weights ./weights/yolov5x.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 512 --batch 2 --epochs 100 --data ./gwdconfig/wheat2.yaml --cfg ./gwdconfig/yolov5x.yaml --name yolov5x_fold2 --weights ./weights/yolov5x.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 512 --batch 2 --epochs 100 --data ./gwdconfig/wheat3.yaml --cfg ./gwdconfig/yolov5x.yaml --name yolov5x_fold3 --weights ./weights/yolov5x.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex\n",
      "{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
      "remote: Enumerating objects: 5, done.\u001B[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001B[K\n",
      "remote: Total 7 (delta 5), reused 5 (delta 5), pack-reused 2\u001B[K\n",
      "Unpacking objects: 100% (7/7), done.\n",
      "From https://github.com/ultralytics/yolov5\n",
      "   37e13f8..0bc80e1  master     -> origin/master\n",
      "Your branch is behind 'origin/master' by 17 commits, and can be fast-forwarded.\n",
      "  (use \"git pull\" to update your local branch)\n",
      "\n",
      "Namespace(adam=False, batch_size=2, bucket='', cache_images=False, cfg='./gwdconfig/yolov5x.yaml', data='./gwdconfig/wheat4.yaml', device='', epochs=100, evolve=False, img_size=[1024], multi_scale=False, name='yolov5x_fold4', noautoanchor=False, nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='./weights/yolov5x.pt')\n",
      "Using CUDA device0 _CudaDeviceProperties(name='GeForce GTX 1080 Ti', total_memory=11178MB)\n",
      "\n",
      "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
      "\n",
      "              from  n    params  module                                  arguments                     \n",
      "  0             -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n",
      "  1             -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
      "  2             -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n",
      "  3             -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
      "  4             -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n",
      "  5             -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
      "  6             -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n",
      "  7             -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
      "  8             -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n",
      "  9             -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n",
      " 10             -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
      " 11             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12        [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13             -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n",
      " 14             -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
      " 15             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16        [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17             -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n",
      " 18             -1  1      5778  torch.nn.modules.conv.Conv2d            [320, 18, 1, 1]               \n",
      " 19             -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
      " 20       [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 21             -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n",
      " 22             -1  1     11538  torch.nn.modules.conv.Conv2d            [640, 18, 1, 1]               \n",
      " 23             -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
      " 24       [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 25             -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n",
      " 26             -1  1     23058  torch.nn.modules.conv.Conv2d            [1280, 18, 1, 1]              \n",
      " 27   [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n",
      "Model Summary: 407 layers, 8.84337e+07 parameters, 8.84337e+07 gradients\n",
      "\n",
      "Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n",
      "/root/anaconda3/envs/python377/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/root/anaconda3/envs/python377/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "Reading image shapes: 100%|███████████████| 2698/2698 [00:01<00:00, 1847.36it/s]\n",
      "Caching labels convertor/fold4/labels/train2017 (2698 found, 0 missing, 0 empty,\n",
      "Saving labels to convertor/fold4/labels/train2017.npy for faster future loading\n",
      "Reading image shapes: 100%|█████████████████| 675/675 [00:00<00:00, 1954.19it/s]\n",
      "Caching labels convertor/fold4/labels/val2017 (675 found, 0 missing, 0 empty, 0 \n",
      "\n",
      "Analyzing anchors... Best Possible Recall (BPR) = 0.9991\n",
      "Image sizes 1024 train, 1024 test\n",
      "Using 2 dataloader workers\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "      0/99     7.89G   0.07112    0.1672         0    0.2383       110      1024\n",
      "               Class      Images     Targets           P           R      mAP@.5\n",
      "                 all         675    2.96e+04       0.357       0.944       0.819       0.346\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "      1/99     7.89G   0.04967     0.154         0    0.2037       123      1024\n",
      "               Class      Images     Targets           P           R      mAP@.5\n",
      "                 all         675    2.96e+04       0.678       0.939       0.929       0.451\n",
      "\n",
      "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
      "      2/99     7.89G   0.04827    0.1473         0    0.1956       113      1024"
     ]
    }
   ],
   "source": [
    "!python train.py --img 512 --batch 2 --epochs 100 --data ./gwdconfig/wheat4.yaml --cfg ./gwdconfig/yolov5x.yaml --name yolov5x_fold4 --weights ./weights/yolov5x.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile    detect.py          results.png                train_batch0.jpg\n",
      "Kaggle.ipynb  \u001B[0m\u001B[01;34mgwdconfig\u001B[0m/         results_yolov5x_fold0.txt  train_batch1.jpg\n",
      "LICENSE       hubconf.py         \u001B[01;34mruns\u001B[0m/                      train_batch2.jpg\n",
      "\u001B[01;32mREADME.md\u001B[0m*    \u001B[01;34minference\u001B[0m/         test.py                    tutorial.ipynb\n",
      "\u001B[01;34m__pycache__\u001B[0m/  labels.png         test_batch0_gt.jpg         \u001B[01;34mutils\u001B[0m/\n",
      "\u001B[01;34mconvertor\u001B[0m/    \u001B[01;34mmodels\u001B[0m/            test_batch0_pred.jpg       \u001B[01;34mweights\u001B[0m/\n",
      "\u001B[01;34mdata\u001B[0m/         \u001B[01;32mrequirements.txt\u001B[0m*  train.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}